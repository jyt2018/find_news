import json
from bs4 import BeautifulSoup
import hashlib
from datetime import datetime
from lxml import etree  # æ–°å¢ï¼šæ”¯æŒ XPath

def parse_article(article_item_node, article_meta_conf):
    """è§£ææ–‡ç« æ¡ç›®èŠ‚ç‚¹ï¼Œæå–æ–‡ç« æ¡ç›®ä¿¡æ¯ï¼Œä½¿ç”¨é…ç½®å‚æ•°2å®šä¹‰æå–è§„åˆ™
    :param article_item_node: æ–‡ç« æ¡ç›®èŠ‚ç‚¹ï¼Œé€šå¸¸æ˜¯ BeautifulSoup å…ƒç´ æˆ– lxml å…ƒç´ ã€‚
    :param article_meta_conf: æ–‡ç« å…ƒä¿¡æ¯é…ç½®å­—å…¸ï¼Œå®šä¹‰äº†æå–æ–‡ç« 7ä¸ªé€šç”¨è¦ç´ çš„è§„åˆ™ã€‚
    :return: articleæå–åˆ°çš„æ–‡ç« æ¡ç›®ä¿¡æ¯å­—å…¸ã€‚
    """

    # åˆå§‹åŒ–æ–‡ç« å­—å…¸, åŒ…å«7ä¸ªé€šç”¨è¦ç´ ,key is same as article_meta_conf and db's column name
    article = {
        "article_url": None,
        "article_title": None,
        "article_tag": None,
        "article_date": None,
        "author": None,
        "excerpt": None,
        "thumbnail": None,
    }

    def apply_filters(elements, filters):
        """
        åº”ç”¨è¿‡æ»¤è§„åˆ™åˆ°å…ƒç´ åˆ—è¡¨ã€‚æ”¯æŒ startswith, not_in
        e.g. {'attr': 'href', 'startswith': '/category/applications/autonomous/'}
        :param elements: å…ƒç´ åˆ—è¡¨ï¼Œé€šå¸¸æ˜¯ BeautifulSoup å…ƒç´ æˆ– lxml å…ƒç´ ã€‚
        :param filters: è¿‡æ»¤è§„åˆ™åˆ—è¡¨ï¼Œæ¯ä¸ªè§„åˆ™æ˜¯ä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å« 'attr' å’Œ 'startswith' æˆ– 'not_in' é”®ã€‚
        :return: è¿‡æ»¤åçš„å…ƒç´ åˆ—è¡¨ã€‚
        """
        for filt in filters:
            attr = filt.get('attr')
            if 'startswith' in filt:
                elements = [el for el in elements if el.get(attr, '').startswith(filt['startswith'])]
            elif 'not_in' in filt:
                elements = [el for el in elements if not any(sub in el.get(attr, '') for sub in filt['not_in'])]
        return elements
    
    def post_process(value, processes, extra=None):
        """
        é’ˆå¯¹stråå¤„ç†ï¼Œæ”¯æŒ 'make_absolute' å’Œ 'strip_prefix' ä¸¤ç§å¤„ç†ã€‚ç”¨äºåœ°å€è¡¥å…¨å’Œå­—ç¬¦ä¸²ç®€å•æ›¿æ¢ã€‚
        :param value: æå–åˆ°çš„å­—æ®µå€¼ï¼Œé€šå¸¸æ˜¯å­—ç¬¦ä¸²ã€‚
        :param processes: åå¤„ç†è§„åˆ™åˆ—è¡¨ï¼Œæ¯ä¸ªè§„åˆ™æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²ï¼ŒåŒ…å« 'make_absolute' æˆ– 'strip_prefix'ã€‚å¦‚æœä¼ å…¥ç©ºåˆ—è¡¨[], åˆ™ä¸è¿›è¡Œåå¤„ç†ã€‚
        :param extra: é¢å¤–å‚æ•°ï¼Œç”¨äº 'strip_prefix' å¤„ç†ï¼ŒåŒ…å« 'prefix' é”®ã€‚
        :return: åå¤„ç†åçš„å­—æ®µå€¼ã€‚
        """

        for proc in processes:
            if proc == 'make_absolute': # ç»å¯¹è·¯å¾„å¤„ç†, è‹¥ä¸æ˜¯ç»å¯¹è·¯å¾„, åˆ™æ·»åŠ ç«™ç‚¹URLå‰ç¼€
                print('hello')
            elif proc.get('replace'): # replace str
                replace_conf = proc.get('replace')
                prefix = replace_conf.get('from', '')
                value = value.replace(prefix, replace_conf.get('to', '')) if value.startswith(prefix) else value

        return value.strip() if isinstance(value, str) else value
    

    def extract_field(context, field_conf): # contextæ˜¯ä¼ å…¥çš„ä¸ƒè¦ç´ ä¹‹ä¸€çš„èŠ‚ç‚¹
        """
        é€’å½’æå–å­—æ®µï¼Œæ”¯æŒåµŒå¥—ã€å›é€€çˆ¶å…ƒç´ ã€é»˜è®¤å€¼ç­‰ã€‚æŒ‰confä»contextä¸­æå–å­—æ®µï¼Œè‹¥æå–å¤±è´¥åˆ™å›é€€åˆ°çˆ¶å…ƒç´ ã€‚
        æå–ä¸€ä¸ªæ–°é—»è¦ç´ ï¼ˆä¸ƒè¦ç´ ä¹‹ä¸€ï¼‰çš„é…ç½®ä¿¡æ¯ï¼Œå¯ä»¥æ˜¯2-6å­—æ®µï¼Œsee sitecfg_key.txt
        :param context: å½“å‰ä¸Šä¸‹æ–‡ï¼Œé€šå¸¸æ˜¯ BeautifulSoup å…ƒç´ æˆ– lxml å…ƒç´ ã€‚æœ€å¼€å§‹ä¼ å…¥<article>å…ƒç´ , åç»­é€’å½’è°ƒç”¨æ—¶ä¼ å…¥<article>çš„å­å…ƒç´ ä¾‹å¦‚<div class="card-body">ã€‚
        :param field_conf: æŠ“å–ç­–ç•¥å­—å…¸ï¼Œå®šä¹‰äº†æå–è§„åˆ™ã€‚å¯¹åº”scrapy_cfg.jsonä¸­çš„extractorsä¸­çš„ä¸€ä¸ªitemçš„valueï¼Œä¾‹å¦‚ï¼š
        {
            "title": {
                "type": "bs4_find",
                "selector": "h2.entry-title a",
                "extract": "text"
            }
        }çš„value
        :return: æå–åˆ°çš„å­—æ®µå€¼ï¼Œæˆ–é»˜è®¤å€¼ã€‚
        """

        if 'parent' in field_conf: # é€’å½’å¤„ç†çˆ¶å…ƒç´ ï¼Œè¿™æ ·å¯ä»¥ç¼©å°æœç´¢èŒƒå›´
            # å¦‚æœæœ‰çˆ¹ï¼Œå…ˆæå–çˆ¹
            parent_conf = field_conf['parent']
            parent = extract_element(context, parent_conf)
            print(f" --- æ‰¾åˆ°äº†ä»–çˆ¹: {parent}\n")
            if not parent:
                return field_conf.get('default', '')
            # å¦‚æœæ‰¾åˆ°äº†çˆ¹, åˆ™ä»¥çˆ¹ä¸ºä¸Šä¸‹æ–‡context, ç»§ç»­æå–
            context = parent
        
        elem = extract_element(context, field_conf)

        extract_key = field_conf.get('extract', 'text') # get extract key, default is 'text'
        print(f" -- extract_key: {extract_key}")
        if extract_key == 'text':
            val = elem.get_text(strip=True)
            print(f" -- text: {val}\n")
        else:
            val = elem.get(extract_key, '')
            print(f" -- val: {val}\n")
        
        processes = field_conf.get('post_process') # å¦‚æœæ²¡æ‰¾åˆ°post_process, åˆ™skip
        if not processes:
            return val

        print(f" -- post_process: {processes}\n")
        val = post_process(val, processes) # processes is list, extra is dict
        print(f" -- post_process: {val}\n")
        if 'sub_selectors' in field_conf:
            for sub in field_conf['sub_selectors']:
                sub_val = extract_field(elem, sub)
                if sub_val and 'key' in sub:
                    article[sub['key']] = sub_val
        
        return val
    
    # è¾…åŠ©å‡½æ•°ï¼šå®šä½å…ƒç´ 
    def extract_element(context, conf):
        # contextè¢«ä¸€çº§è°ƒç”¨æ—¶, ä¼ å…¥çš„æ˜¯<article>å…ƒç´ , åç»­é€’å½’è°ƒç”¨æ—¶ä¼ å…¥çš„æ˜¯<article>çš„å­å…ƒç´ ä¾‹å¦‚<div class="card-body">
        typ = conf['type'] # æå–æ‰‹æ®µï¼Œä¾‹å¦‚ bs4_find, bs4_find_all, xpath ç­‰
        print(f" -- type: {typ}")
        sel = conf['selector'] # æå–ç›®æ ‡ï¼Œä¾‹å¦‚ 'h2.entry-title a' or ['h2', {'class': 'entry-title'}] or {'class_': 'card-bocy'} 
        print(f" -- selector: {sel}")
        
        # type ä¸‰é€‰ä¸€
        if typ == 'bs4_find':
            # å¦‚æœæ˜¯å­—å…¸ç±»å‹åˆ™ä½¿ç”¨**selè§£åŒ…å­—å…¸ï¼Œå½¢æˆcontext.find(name='div', class_='card-bocy')
            if isinstance(sel, dict):
                return context.find(**sel)
            elif isinstance(sel, list): # å¦‚æœæ˜¯åˆ—è¡¨ç±»å‹åˆ™ä½¿ç”¨*selè§£åŒ…åˆ—è¡¨ï¼Œå½¢æˆcontext.find('h2', {'class': 'entry-title'})
                return context.find(*sel)
            else: # å¦‚æœæ˜¯å­—ç¬¦ä¸²ç±»å‹åˆ™ç›´æ¥ä½¿ç”¨selï¼Œå½¢æˆcontext.find('h2.entry-title a')
                return context.find(sel)
        elif typ == 'bs4_find_all':
            # å¦‚æœæ˜¯å­—å…¸ç±»å‹åˆ™ä½¿ç”¨**selè§£åŒ…å­—å…¸ï¼Œå½¢æˆcontext.find_all(name='div', class_='card-bocy')
            if isinstance(sel, dict):
                elems = context.find_all(**sel)
            elif isinstance(sel, list): # å¦‚æœæ˜¯åˆ—è¡¨ç±»å‹åˆ™ä½¿ç”¨*selè§£åŒ…åˆ—è¡¨ï¼Œå½¢æˆcontext.find_all('h2', {'class': 'entry-title'})
                elems = context.find_all(*sel)
            else: # å¦‚æœæ˜¯å­—ç¬¦ä¸²ç±»å‹åˆ™ç›´æ¥ä½¿ç”¨selï¼Œå½¢æˆcontext.find_all('h2.entry-title a')
                elems = context.find_all(sel)

            # find_allæ•°é‡å¤šä¼šæ¶‰åŠåˆ°è¿‡æ»¤
            print(f" -- å«æœ‰{sel}: {len(elems)}ä¸ª")
            if 'filters' in conf:
                elems = apply_filters(elems, conf['filters'])
                # print(f" -- elems after filters: {elems}\n")
            limit = conf.get('limit', None)
            return elems[0] if limit == 1 else elems
        elif typ == 'xpath':
            elems = tree.xpath(sel) if context is soup else context.xpath(sel)  # lxml æ”¯æŒç›¸å¯¹ XPath
            print(f" -- elems: {elems}\n")
            return elems[0] if elems else None
        return None
    
    # ===========================================
    # parse_article ä¸»å¹²-ä¾æ¬¡æå–7è¦ç´ , æœ€å¤šå¾ªç¯7æ¬¡
    for field, field_conf in article_meta_conf.items():
        # field is the key in config(eg. title, url, author ), type is str; mapping to db columns
        # field_conf is æ–‡ç« ä¸ƒè¦ç´ çš„ä¸€ä¸ªè¦ç´ çš„é…ç½®å­—å…¸
        print(f"âœ”ï¸ ä¸ƒè¦ç´     : {field}")
        print(f"âœ”ï¸ ä¸ƒè¦é…conf: {field_conf}\n")

        # æœ€å¼€å§‹ä¼ å…¥<article>
        article[field] = extract_field(article_item_node, field_conf) # article['title'] = 'title text'
        print(f" == {field}: {article[field]}\n")

    
    return article

if __name__ == '__main__':
    # å¯¼å…¥è‡ªå®šä¹‰åº“
    from lib_get_articletag import get_article_containers
    
    html_path = 'index2.html'  # ç›´æ¥ä½¿ç”¨å½“å‰ç›®å½•çš„æ–‡ä»¶ï¼Œå› ä¸ºè„šæœ¬åœ¨toolsç›®å½•è¿è¡Œ

    config_page = 'scrapy_page.json'
    config_meta = 'scrapy_article.json'

    c_tag = 'autonomous'
    
    # åŠ è½½é¡µé¢é…ç½®æ–‡ä»¶ æ‰¾articleåœ¨å“ª
    with open(config_page, 'r', encoding='utf-8') as f:
        config_article = json.load(f)
        print(config_article.keys())


    # åŠ è½½é…ç½®æ–‡ä»¶
    with open(config_meta, 'r', encoding='utf-8') as f2:
        config = json.load(f2)
    article_meta_conf = config['article_meta'] # æ–‡ç« å…ƒä¿¡æ¯é…ç½®ï¼Œç”¨äºæå–æ–‡ç« çš„7ä¸ªé€šç”¨è¦ç´ 
    
    # è¯»å–HTMLæ–‡ä»¶
    with open(html_path, 'r', encoding='utf-8') as f3:
        html_content = f3.read()
    
    # ç¬¬ä¸€æ­¥ï¼šä½¿ç”¨è‡ªå®šä¹‰åº“è·å–æ–‡ç« å®¹å™¨
    try:

        article_containers = get_article_containers(html_content, config_article)
        print(len(article_containers), "ç¯‡\n")
    except ValueError as e:
        print(f"é”™è¯¯: {e}")
        exit(1)
    except Exception as e:
        print(f"å¤„ç†è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        exit(1)

    # ç¬¬äºŒæ­¥ï¼Œéå†æ¯ä¸ªæ–‡ç« æ¡ç›®ï¼Œæå–å„ä¸ªæ–‡ç« æ¡ç›®çš„é€šç”¨7è¦ç´ element(æ ‡é¢˜ ä½œè€… æ—¥æœŸ æ‘˜è¦ é“¾æ¥ åˆ†ç±»æ ‡ç­¾ å°å›¾ç‰‡)
    for article_item_node in article_containers:
        # æ¯æ¬¡å¤„ç†ä¸€ç¯‡æ–‡ç« 
        article_item = parse_article(article_item_node, article_meta_conf) # article_meta_confæ˜¯å­—å…¸ï¼Œæœ‰7ä¸ªitemå¯¹åº”æ–‡ç« çš„ä¸ƒè¦ç´ æå–ç­–ç•¥
        article_item['tag'] = c_tag # c_tagçš„ä½œç”¨æ˜¯æŠŠç‰ˆå—ä¿¡æ¯ä¼ é€’ç»™singleæ–‡ç« ï¼Œä»¥åç”¨è¿™ä¸ªå­—æ®µæ‹¼æ¥æ‰€æœ‰tag
        print("\n")
        # print("ğŸ–ï¸ ", article_item, "\n\n")

